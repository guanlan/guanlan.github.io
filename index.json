[{"categories":null,"content":"Protecting Services With Kong Gateway Rate Limiting The Kong Gateway Rate Limiting plugin is one of our most popular traffic control add-ons. You can configure the plugin with a policy for what constitutes “similar requests” (requests coming from the same IP address, for example), and you can set your limits (limit to 10 requests per minute, for example). This tutorial will walk through how simple it is to enable rate limiting in your Kong Gateway. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:0:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Rate Limiting: Protecting Your Server 101 Let’s take a step back and go over the concept of rate limiting for those who aren’t familiar. Rate limiting is remarkably effective and ridiculously simple. It’s also regularly forgotten. Rate limiting is a defensive measure you can use to prevent your server or application from being paralyzed. By restricting the number of similar requests that can hit your server within a window of time, you ensure your server won’t be overwhelmed and debilitated. You’re not only guarding against malicious requests. Yes, you want to shut down a bot that’s trying to discover login credentials with a brute force attack. You want to stop scrapers from slurping up your content. You want to safeguard your server from DDOS attacks. But it’s also vital to limit non-malicious requests. Sometimes, it’s somebody else’s buggy code that hits your API endpoint 10 times a second rather than one time every 10 minutes. Or, perhaps only your premium users get unlimited API requests, while your free-tier users only get a hundred requests an hour. If you’re interested in rate limiting for Kubernetes services, check out this video: ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:1:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Mini-Project for Kong Gateway Rate Limiting In our mini-project for this article, we’re going to walk through a basic use case: Kong Gateway with the Rate Limiting plugin protecting a simple API server. Here are our steps: Create Node.js Express API server with a single “hello world” endpoint. Install and set up Kong Gateway. Configure Kong Gateway to sit in front of our API server. Add and configure the Rate Limiting plugin. Test our rate limiting policies. After walking through these steps together, you’ll have what you need to tailor the Rate Limiting plugin for your unique business needs. Want to set up rate limiting for your API gateway with clicks instead of code? Try Konnect for free » ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:2:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Create a Node.js Express API Server To get started, we’ll create a simple API server with a single endpoint that listens for a GET request and responds with “hello world.” At the command line, create a project folder and initialize a Node.js application: ~/$ mkdir project ~/$ cd project ~/project$ yarn init // Accept all defaults Then, we’ll add Express, which is the only package we’ll need: ~/project$ yarn add express Lastly, let’s create a simple server with our “hello world” endpoint. In your project folder, create an index.js file with these contents: /* PATH: ~/project/index.js */ const express = require('express') const server = express() const port = 3000 server.get('/', (req, res) =\u003e { console.log(req.headers) res.status(200).send('Hello world!') }) server.listen(port, () =\u003e { console.log(`Server is listening on http://localhost:${port}`) }) Now, spin up your server: ~/project$ node index.js In your browser, you can visit http://localhost:3000. Here’s what you should see: Let’s also use Insomnia, a desktop client for API testing. In Insomnia, we send a GET request to http://localhost:3000. Our Node.js API server with its single endpoint is up and running. We have our 200 OK response. Keep that terminal window with the node running. We’ll do the rest of our work in a separate terminal window. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:3:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Install and Set Up Kong Gateway Next, we’ll install Kong Gateway to sit in front of our API server. These steps will vary depending on your local environment. Simple usage of the Rate Limiting plugin supports configuring Kong in DB-less mode with a declarative configuration. That means, instead of sending configurations to Kong Gateway one step at a time, with those configurations stored in a database, we can use a single declarative .yml file for specifying our entire Kong configuration. After installing Kong, we generate a starter .yml file: ~/project$ kong config init ~/project$ tree -L 1 . ├── index.js ├── kong.yml ├── node_modules ├── package.json └── yarn.lock 1 directory, 4 files We’ll come back to that kong.yml file in a moment. Now, let’s tell Kong where to look for that declarative configuration file upon startup. In /etc/kong, there is a kong.conf.default file that we’ll need to copy as kong.conf and then edit: ~/project$ cd /etc/kong /etc/kong$ sudo su root:/etc/kong$ cp kong.conf.default kong.conf Next, we edit the kong.conf file. You’ll likely need root privileges to do this. There are two edits that we need to make: # PATH: /etc/kong/kong.conf # line ~839: Uncomment this line and set to off database = off # line ~1023, Uncomment this line. Set to absolute path to kong.yml declarative_config = /PATH/TO/YOUR/project/kong.yml ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:4:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Configure Kong Gateway With Our Service and Routes Before we start up Kong, let’s edit the declarative configuration file generated in our project folder. It should look like this: # PATH: ~/project/kong.yml_format_version:\"2.1\"services:- name:my-api-serverurl:http://localhost:3000/routes:- name:api-routespaths:- /api Let’s walk through what this configuration does. After setting the syntax version (2.1), we configure a new upstream service for which Kong will serve as an API gateway. Our service, which we name my-api-server, listens for requests at the URL http://localhost:3000. We associate a route (arbitrarily named api-routes) for our service with the path /api. Kong Gateway will listen for requests to /api, and then route those requests to our API server at http://localhost:3000. With our declarative configuration file in place, we start Kong: ~/project$ sudo kong start Now, in Insomnia, we send a GET request through Kong Gateway, which listens at http://localhost:8000, to the /api path: Note that, by sending our request to port 8000, we are going through Kong Gateway to get to our API server. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:5:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Add the Kong Gateway Rate Limiting Plugin Now that we have Kong Gateway sitting in front of our API server, we’ll add in the Rate Limiting Plugin and test it out. We will need to add a few lines to our kong.yml declarative configuration file: # PATH: ~/project/kong.yml_format_version:\"2.1\"services:- name:my-api-serverurl:http://localhost:3000/routes:- name:api-routespaths:- /apiplugins:- name:rate-limitingconfig:minute:5hour:12policy:locallimit_by:headerheader_name:x-api-keypolicy:local We’ve added the entire plugins section underneath our my-api-server service. We specify the name of the plugin, rate-limiting. This name is not arbitrary but refers to the actual rate-limiting plugin in the Kong package. In this first run, we’ve configured the plugin with minute: 5, which allows for up to five requests per minute. We’ve also added hour : 12, which limits the requests per hour to 12. We’re using the local policy, which has to do with how Kong will store and increment request counts. We’ll talk about this policy configuration more below. Let’s restart Kong: ~/project$ sudo kong restart Now, back in Insomnia, we test out sending some requests to our API endpoint. If you send a request every few seconds, you’ll see that the first five requests received a 200 OK response. However, if you exceed five requests within a minute: The Rate Limiting plugin detects that requests have exceeded the five-per-minute rule. It doesn’t let the subsequent request through to the API server. Instead, we get a 429 Too Many Requests response. Our plugin works! If you wait a minute and then try your requests again, you’ll see that you can get another five successful requests until Kong blocks you again with another 429. At this point, we’ve sent 10 requests over several minutes. But you’ll recall that we configured our plugin with hour: 12. That means our 11th and 12th requests will be successful. However, the system will reject our 13th request even though we haven’t exceeded the five-per-minute rule . That’s because we’ll have exceeded the 12-per-hour rule. The Rate Limiting plugin allows you to limit the number of requests per second, minute, hour, day, month and year. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:6:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Rate Limiting “Similar” Requests We’ve configured Kong to count (and limit) requests to our server in our simple use case so far. More likely, we’ll want to apply our rate limits to similar requests. By “similar,” we might mean “coming from the same IP address” or “using the same API key.” For example, let’s say that all users of our API server need to send requests with a unique API key set in headers as x-api-key. We can configure Kong to apply its rate limits on a per-API-key basis as follows: # PATH: ~/project/kong.yml_format_version:\"2.1\"services:- name:my-api-serverurl:http://localhost:3000/routes:- name:api-routespaths:- /apiplugins:- name:rate-limitingconfig:minute:5hour:12policy:local If we change the x-api-key to a different value (for example, user2), we immediately get 200 OK responses: Requests from user2 are counted separately from requests from user1. Each unique x-api-key value gets (according to our rate limiting rules) up to five requests per minute and up to 12 requests per hour. You can configure the plugin to limit_by IP address, a header value, request path or even credentials (like OAuth2 or JWT). ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:6:1","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Counter Storage “Policy” In our example above, we set the policy of the Rate Limiting plugin to local. This policy configuration controls how Kong will store request counters to apply its rate limits. The local policy stores in-memory counters. It’s the simplest strategy to implement (there’s nothing else you need to do!). However, request counters with this strategy are only mostly accurate. If you want basic protection for your server, and it’s acceptable if you miscount a request here and there, then the local policy is sufficient. The documentation for the Rate Limiting plugin instructs those with needs where “every transaction counts” to use the cluster (writes to the database) or redis (writes to Redis key-value store) policy instead. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:6:2","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"Advanced Rate Limiting For many common business cases, the open source Rate Limiting plugin has enough configuration options to meet your needs. For those with more complex needs (multiple limits or time windows, integration with Redis Sentinel), Kong also offers their Rate Limiting Advanced plugin for Kong Konnect. Let’s briefly recap what we did in our mini-project. We spun up a simple API server. Then, we installed and configured Kong Gateway to sit in front of our API server. Next, we added the Rate Limiting plugin to count and limit requests to our server, showing how Kong blocks requests once we exceed certain limits within a window of time. Lastly, we demonstrated how to group (and count) requests as “similar” with the example of a header value. Doing so enabled our plugin to differentiate counts for requests coming from two different users so that you can apply that rate limit to each user separately. That’s all there is to it. Kong’s Rate Limiting plugin makes protecting your server ridiculously simple. As a basic traffic control defense measure, rate limiting can bring incredible power and peace of mind. You can find more details in the rate limiting plugin doc. If you have any additional questions, post them on Kong Nation. To stay in touch, join the Kong Community. ","date":"2021-05-18","objectID":"/kong-gateway-rate-limiting/:7:0","tags":["Kong","Rate Limiting"],"title":"Protecting Services With Kong Gateway Rate Limiting","uri":"/kong-gateway-rate-limiting/"},{"categories":null,"content":"What is rate limiting? Rate limiting protects your APIs from inadvertent or malicious overuse by limiting how often each user can call the API. Without rate limiting, each user may make a request as often as they like, leading to “spikes” of requests that starve other consumers. Once enabled, rate limiting can only perform a fixed number of requests per second. A rate limiting algorithm helps automate the process. In the example chart, you can see how rate limiting blocks requests over time. The API was initially receiving four requests per minute, shown in green. When we enabled rate limiting at 12:02, the system denied additional requests, shown in red. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:1:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Why is rate limiting used? Rate limiting is very important for public APIs where you want to maintain a good quality of service for every consumer, even when some users take more than their fair share. Computationally-intensive endpoints are particularly in need of rate limiting — especially when served by auto-scaling, or by pay-by-the-computation services like AWS Lambda. You also may want to rate limit APIs that serve sensitive data because this could limit the data exposed if an attacker gains access in some unforeseen event. Rate limiting can be used to accomplish the following: Avoid resource starvation: Rate limiting can improve the availability of services and help avoid friendly-fire denial-of-service (DoS) attacks. Cost management: Rate limiting can be used to enforce cost controls, (for example) preventing surprise bills from experiments or misconfigured resources . Manage policies and quotas: Rate limiting allows for the fair sharing of a service with multiple users. Control flow: For APIs that process massive amounts of data, rate limiting can be used to control the flow of that data. It can allow for the merging of multiple streams into one service or the equally distribution of a single stream to multiple workers. Security: Rate limiting can be used as defense or mitigation against some common attacks. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:2:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"What kind of attacks can rate limiting prevent? Rate limiting can be critical in protecting against a variety of bot-based attacks, including: DoS and DDoS (distributed denial-of-service) attacks Brute force and credential stuffing attacks Web scraping or site scraping ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:3:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"How do you enable rate limiting? There are many different ways to enable rate limiting, and we will explore the pros and cons of varying rate limiting algorithms. We will also explore the issues that come up when scaling across a cluster. Lastly, we’ll show you an example of how to quickly set up rate limiting using Kong Gateway, which is the most popular open-source API gateway. Want to set up rate limiting for your API gateway with clicks instead of code? Try Konnect for free » If you’re interested in rate limiting for Kubernetes services, check out this video: ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:4:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Rate limiting algorithms There are various algorithms for API rate limiting, each with its benefits and drawbacks. Let’s review each of them so you can pick the ideal rate limiting design option for your API needs. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:5:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Leaky Bucket Leaky bucket (closely related to token bucket) is an algorithm that provides a simple, intuitive approach to rate limiting via a queue, which you can think of as a bucket holding the requests. When registering a request, the system appends it to the end of the queue. Processing for the first item on the queue occurs at a regular interval or first in, first out (FIFO). If the queue is full, then additional requests are discarded (or leaked). This algorithm’s advantage is that it smooths out bursts of requests and processes them at an approximately average rate. It’s also easy to implement on a single server orload balancer and is memory efficient for each user, given the limited queue size. However, a burst of traffic can fill up the queue with old requests and starve more recent requests from being processed. It also provides no guarantee that requests get processed in a fixed amount of time. Additionally, if you load balance servers for fault tolerance or increased throughput, you must use a policy to coordinate and enforce the limit between them. We will come back to the challenges of distributed environments later. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:5:1","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Fixed Window The system uses a window size of n seconds (typically using human-friendly values, such as 60 or 3600 seconds) to track the fixed window algorithm rate. Each incoming request increments the counter for the window. It discards the request if the counter exceeds a threshold. The current timestamp floor typically defines the windows, so 12:00:03, with a 60-second window length, would be in the 12:00:00 window. This algorithm’s advantage is that it ensures more recent requests get processed without being starved by old requests. However, a single burst of traffic that occurs near the boundary of a window can result in the processing of twice the rate of requests because it will allow requests for both the current and next windows within a short time. Additionally, if many consumers wait for a reset window, they may stampede your API at the same time at the top of the hour. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:5:2","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Sliding Log Sliding Log rate limiting involves tracking a time-stamped log for each consumer’s request. The system stores these logs in a time-sorted hash set or table. It also discards logs with timestamps beyond a threshold. When a new request comes in, we calculate the sum of logs to determine the request rate. If the request would exceed the threshold rate, then it is held. The advantage of this algorithm is that it does not suffer from the boundary conditions of fixed windows. Enforcement of the rate limit will remain precise. Since the system tracks the sliding log for each consumer, you don’t have the stampede effect that challenges fixed windows. However, it can be costly to store an unlimited number of logs for every request. It’s also expensive to compute because each request requires calculating a summation over the consumer’s prior requests, potentially across a cluster of servers. As a result, it does not scale well to handle large bursts of traffic or denial of service attacks. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:5:3","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Sliding Window Sliding Window is a hybrid approach that combines the fixed window algorithm’s low processing cost and the sliding log’s improved boundary conditions. Like the fixed window algorithm, we track a counter for each fixed window. Next, we account for a weighted value of the previous window’s request rate based on the current timestamp to smooth out bursts of traffic. For example, if the current window is 25% through, we weigh the previous window’s count by 75%. The relatively small number of data points needed to track per key allows us to scale and distribute across large clusters. We recommend the sliding window approach because it gives the flexibility to scale rate limiting with good performance. The rate windows are an intuitive way to present rate limit data to API consumers. It also avoids the starvation problem of the leaky bucket and the bursting problems of fixed window implementations. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:5:4","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Rate limiting in distributed systems ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:6:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Synchronization Policies If you want to enforce a global rate limit when using a cluster of multiple nodes, you must set up a policy to enforce it. If each node were to track its rate limit, a consumer could exceed a global rate limit when sending requests to different nodes. The greater the number of nodes, the more likely the user will exceed the global limit. The simplest way to enforce the limit is to set up sticky sessions in your load balancer so that each consumer gets sent to exactly one node. The disadvantages include a lack of fault tolerance and scaling problems when nodes get overloaded. A better solution that allows more flexible load-balancing rules is to use a centralized data store such as Redis orCassandra. A centralized data store will collect the counts for each window and consumer. The two main problems with this approach are increased latency making requests to the data store and race conditions, which we will discuss next. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:6:1","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Race Conditions One of the most extensive problems with a centralized data store is the potential for race conditions in high concurrency request patterns. This issue happens when you use a naïve “get-then-set” approach, wherein you retrieve the current rate limit counter, increment it, and then push it back to the datastore. This model’s problem is that additional requests can come through in the time it takes to perform a full cycle of read-increment-store, each attempting to store the increment counter with an invalid (lower) counter value. This allows a consumer to send a very high rate of requests to bypass rate limiting controls. One way to avoid this problem is to put a “lock” around the key in question, preventing any other processes from accessing or writing to the counter. A lock would quickly become a significant performance bottleneck and does not scale well, mainly when using remote servers like Redis as the backing datastore. A better approach is to use a “set-then-get” mindset, relying on atomic operators that implement locks in a very performant fashion, allowing you to quickly increment and check counter values without letting the atomic operations get in the way. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:6:2","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Optimizing for Performance The increased latency is another disadvantage of using a centralized data store when checking the rate limit counters. Unfortunately, even checking a fast data store like Redis would result in milliseconds of additional latency for every request. Make checks locally in memory to make these rate limit determinations with minimal latency. To make local checks, relax the rate check conditions and use an eventually consistent model. For example, each node can create a data sync cycle that will synchronize with the centralized data store. Each node periodically pushes a counter increment for each consumer and window to the datastore. These pushes atomically update the values. The node can then retrieve the updated values to update its in-memory version. This cycle of converge → diverge → reconverge among nodes in the cluster is eventually consistent. The periodic rate at which nodes converge should be configurable. Shorter sync intervals will result in less divergence of data points when spreading traffic across multiple nodes in the cluster (e.g., when sitting behind a round robin balancer). Whereas longer sync intervals put less read/write pressure on the datastore and less overhead on each node to fetch new synced values. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:6:3","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Quickly set up rate limiting with Kong API Gateway Kong is an open source API gateway that makes it very easy to build scalable services with rate limiting. It scales perfectly from single Kong Gateway nodes to massive, globe-spanning Kong clusters. Kong Gateway sits in front of your APIs and is the main entry point to your upstream APIs. While processing the request and the response, Kong Gateway will execute any plugin that you have decided to add to the API. Kong API Gateway’s rate limiting plug-in is highly configurable. It: Offers flexibility to define multiple rate limit windows and rates for each API and consumer Includes support for local memory, Redis, Postgres, and Cassandra backing datastores Offers a variety of data synchronization options, including synchronous and eventually consistent models You can quickly try Kong Gateway on one of your dev machines to test it out. My favorite way to get started is to use the AWS cloud formation template since I get a pre-configured dev machine in just a few clicks. Just choose one of the HVM options, and set your instance sizes to use t2.micro as these are affordable for testing. Then ssh into a command line on your new instance for the next step. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:7:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Adding an API on Kong Gateway The next step is adding an API on Kong Gateway using the admin API, which can be done by setting up a Route and a Service entity. We will use httpbin, a free testing service for APIs, as our example upstream service,. The get URL will mirror back my request data as JSON. We also assume Kong Gateway is running on the local system at the default ports. curl -i -X POST --url http://localhost:8001/services/ --data 'name=httpbin --data 'url=http://httpbin.org/get' curl -i -X POST --url http://localhost:8001/services/httpbin/routes --data 'paths[]=/test' Now Kong Gateway is aware that every request sent to “/test” should be proxied to httpbin. We can make the following request to Kong Gateway on its proxy port to test it: curl http://localhost:8000/test { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Connection\": \"close\", \"Host\": \"httpbin.org\", \"User-Agent\": \"curl/7.51.0\", \"X-Forwarded-Host\": \"localhost\" }, \"origin\": \"localhost, 52.89.171.202\", \"url\": \"http://localhost/get\" } It’s alive! Kong Gateway received the request and proxied it to httpbin, which has mirrored back the headers for my request and my origin IP address. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:7:1","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Adding Basic Rate-Limiting Let’s go ahead and protect it from an excessive number of requests by adding the rate-limiting functionality using the community edition Rate-Limiting plugin, with a limit of 5 requests per minute from every consumer: curl -i -X POST http://localhost:8001/services/httpbin/plugins/ -d \"name=rate-limiting\" -d \"config.minute=5\" If we now make more than five requests, Kong Gateway will respond with the following error message: curl http://localhost:8000/test { \"message\":\"API rate limit exceeded\" } Looking good! We have added an API on Kong Gateway, and we added rate-limiting in just two HTTP requests to the admin API. It defaults to rate limiting by IP address using fixed windows and synchronizes across all nodes in your cluster using your default datastore. For other options, including rate limiting per consumer or using another datastore like Redis, please see the documentation. For a more detailed tutorial, check out Protecting Services With Kong Gateway Rate Limiting » ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:7:2","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"Better performance with advanced rate limiting and Konnect The Advanced Rate Limiting plugin adds support for the sliding window algorithm for better control and performance. The sliding window prevents your API from being overloaded near window boundaries, as explained in the sections above. For low latency, it uses an in-memory table of the counters and can synchronize across the cluster using asynchronous or synchronous updates. This gives the latency of local thresholds and is scalable across your entire cluster. The first step is start a free trial of Konnect. You can then configure the rate limit, the window size in seconds, and how often to sync the counter values. It’s straightforward to use, and you can get this robust control with a simple API call: curl -i -X POST http://localhost:8001/services/httpbin/plugins -d \"name=rate-limiting\" -d \"config.limit=5\" -d \"config.window_size=60\" -d \"config.sync_rate=10\" The enterprise edition also supports Redis Sentinel, which makes Redis highly available and more fault-tolerant. You can read more in the rate limiting plugin documentation. Other features include an admin GUI, more security features like role-based access control, analytics and professional support. If you’re interested in learning more about the Enterprise edition, just contact Kong’s sales team to request a demo. ","date":"2021-01-15","objectID":"/how-to-design-a-scalable-rate-limiting-algorithm/:8:0","tags":["Kong","Rate Limiting"],"title":"How to Design a Scalable Rate Limiting Algorithm","uri":"/how-to-design-a-scalable-rate-limiting-algorithm/"},{"categories":null,"content":"A web crawler is a hard-working bot to gather information or index the pages on the Internet. It starts at some seeds URLs and finds every hyperlink on each page, and then crawler will visit those hyperlinks recursively. ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:0","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"1. Choose an Ideal Programming Language Here is a ranking of popular languages on developing web crawlers (based on result numbers of relative repositories host on Github on February, 2013): Python or Ruby probably is a wise choice, the mainly speed limit of web crawler is network latency not CPU, so choose Python or Ruby as a language to develop a web crawler will make life easier. Python provide some standard libraries, they are very useful, such like urllib, httplib and regex, those libraries can handle lots of work. Python also has plenty of valuable third-party libraries worth a try: scrapy, a web scraping framework. urllib3, a Python HTTP library with thread-safe connection pooling, file post support. greenlet, a Lightweight concurrent programming framework. twisted, an event-driven networking engine. ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:1","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"2. Reading Some Simple Open-source Projects You need to figure out how exactly does a crawler works. Here is a very simple crawler written in Python, in 10 lines of code. [python] import re, urllib crawled_urls = set() def crawl(url): for new_url in re.findall(‘‘‘href=[\"’](.[^\"’]+)[\"’]’’’, urllib.urlopen(url).read()): if new_url not in crawled_urls: print new_url crawled_urls.add(new_url) if __name__ == “__main__”: url = ‘http://www.yahoo.com/' crawl(url) [/python] Crawler usually needs to keep track of which URLs need to be crawled, and which URLs has already crawled (to avoid the infinite loop). Other simple projects: Python Crawler, a very simple crawler using Berkeley DB to store results. pholcidae, a tiny Python module allows you to write your own crawl spider fast and easy. ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:2","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"3.Choosing the Right Data Structure Choosing a proper data structure will make your crawler efficiently. Queue or Stack is a good choice to store the URLs need be crawled, Hash table or R-B tree seems proper for tracking the crawled URLs, it provide a fast speed to search. Search Time Complexity: Hash table O(1), R-B Tree O(log n) But what if your crawler needs to deal with tons of URLs your memory is not enough? Try to store the checksum of URL string, if it still not enough, you may need to use the Cache algorithms (such like LRU) to dump some URLs into the disk. ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:3","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"4. Multithreading and Asynchronous If you crawling sites from different servers, using multithreading or asynchronous mechanism will save you lots of time. Remember keep your crawler thread-safe, you need a thread-safe queue to share the results and a thread controller to handle threads. Asynchronous is a event-based mechanism will make your crawler enter a while loop, when an events triggers (some resources become available), your crawler will wake up to do deal with this event (usually by execute callback function), Asynchronous can improve throughput, latency of your crawler. Related Resources: How to write a multi-threaded webcrawler, Andreas Hess ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:4","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"5. HTTP Persistent Connections Every time sends an HTTP request you need to open a TCP socket connection, when you finish request, this socket will be closed. When you crawl lots of pages on a same server, you will open and close the socket again and again. The overhead cost is quite a big problem. [code]Connection: Keep-Alive[/code] Use this header in your HTTP request to tell the server your client support keep-alive. Your code also should be modified accordingly. ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:5","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"6. Efficient Regular Expressions You should really figure out how the regex works, a good regex really makes a difference in performance. When your web crawlers parsing the information of the HTTP response, the same regex will execute frequently. Compile a regex need little more time in the beginning, but it will run faster when you use it. Notice if you are using Python (or .NET), it will automatically compile and cache the regexs, but it may still be worthwhile to manually do it, you can give it a proper name after compiling a regex, it will make your code more readable. If you want parser even faster, you probably need to write a parser by yourself. Related Resources: Mastering Regular Expressions, Third Edition by Jeffrey Friedl. Performance of Greedy vs. Lazy Regex Quantifiers, Steven Levithan Optimizing regular expressions in Java, Cristian Mocanu ","date":"2013-02-19","objectID":"/effective-web-crawler/:0:6","tags":["Networking"],"title":"Practical Tips on Writing an Effective Web Crawler","uri":"/effective-web-crawler/"},{"categories":null,"content":"In the CS 5010 - Programming Design Paradigm course, we need write tons of documents, repeat ourselves again and again, so I develop this snippet to save us time. If you using vim, you need install snipMate first, You can get the snippet on my Github: https://github.com/guanlan/snippet-for-racket download the snippet file and put in ~/.vim/snippets/ You can watch demo here This snippets is also available on sublime, you can read tutorial here. ","date":"2013-02-03","objectID":"/snippets-for-racket-language/:0:0","tags":["Lisp"],"title":"Snippets for Racket Language","uri":"/snippets-for-racket-language/"},{"categories":null,"content":"After about 3 months to learn and using Racket (a programming language in Lisp/Scheme family), I learning lots of concepts of programming. The most important thing in FL(functional languages) is: \"All data are immutable. All functions are pure.\" ","date":"2013-01-09","objectID":"/rethink-in-functional-languages/:0:0","tags":["Lisp"],"title":"Rethink in Functional Languages","uri":"/rethink-in-functional-languages/"},{"categories":null,"content":"Immutable Data Immutable data cannot be modified after being created. It has many advantages: Inherently Thread safety Parallel programming is the nightmare of some programers, because different threads simultaneously access the same object can cause unexpected problem, such like a race conditions. The most famous and classical example is Bank Account Problem. Today we get more and more cores in our computers, so the multi-core crisis we need to face, some experts believe in the future, we using Scala or Erlang to deal with this crisis.Interesting thing is many “next generation” language is either the functional or support functional paradigm. Eliminate side-effects There are so many reasons. Most important thing is global variables are difficult to understand. For example, if we want understand one functions in the non-trival project, and we find a variable define in other place, so we will jump to the definition of this variable and other places where modify it, it will cost us more time to figure out what the exactly meaning of this function. So there is the same reason why the code is much easier to understand if we use immutable objects, because the scope of an immutable object is limited as possible. It will make our programming easier and more robust.  The programming experts from other languages also know we need use immutable data as more as possible: Joshua Bloch,the author of Effective Java, said, “If an object is immutable, it can be in only one state, and you win big.You never have to worry about what state the object is in, and you can share it freely, with no need for synchronization.” Scott Meyers, Effective C++, Item 3: Use const whenever possible. Other languages also use immutable data frequently, such like Python, the number/string/tuple types are immutable data, the String build-in class in Java is immutable too. But immutable data is inconvenient in some situation: Sometime we need our objects to share information with other objects that it doesn’t know about. If this shared object is immutable, it will create a new object when modify its data, but other object still gets the information from this old object, there will be trouble. Because every time we change our data, we need create a new data for it, it will cause wasting more storage in our memory, we need use some mechanism to deal with this problem, the string object in Python/Java using String Interning mechanism (e.g. implement of flyweight design pattern) to solve this problem. It will bring us more complexity.  ","date":"2013-01-09","objectID":"/rethink-in-functional-languages/:1:0","tags":["Lisp"],"title":"Rethink in Functional Languages","uri":"/rethink-in-functional-languages/"},{"categories":null,"content":"Pure function All functions are pure, it’s mean one function only do one thing, As we know, the most valuable quote in Unix philosophy is “Do one thing and do it well”. It will make program more readable and reusable. ","date":"2013-01-09","objectID":"/rethink-in-functional-languages/:2:0","tags":["Lisp"],"title":"Rethink in Functional Languages","uri":"/rethink-in-functional-languages/"},{"categories":null,"content":"Recursion Iteration in function language is not well supported, because a loop counter would constitute mutable state, so recursion is heavily used. If you need a for loop in Scheme, you need build it by yourself [code](define-syntax while (syntax-rules () ((_ pred b1 …) (let loop () (when pred b1 … (loop)))))) (define-syntax for (syntax-rules () ((_ (i from to) b1 …) (let loop((i from)) (when (\u003c i to) b1 … (loop (1+ i)))))))[/code] Functional language often use tail call optimization to ensure that heavy recursion does not consume excessive memory. ","date":"2013-01-09","objectID":"/rethink-in-functional-languages/:3:0","tags":["Lisp"],"title":"Rethink in Functional Languages","uri":"/rethink-in-functional-languages/"},{"categories":null,"content":"Lisp has some very effective way to get jobs done, this article give you a direct way to use Python like Lisp.``` cons = lambda el, lst: (el, lst) mklist = lambda *args: reduce(lambda lst, el: cons(el, lst), reversed(args), None) car = lambda lst: lst[0] if lst else lst cdr = lambda lst: lst[1] if lst else lst nth = lambda n, lst: nth(n-1, cdr(lst)) if n ]]\u003e 0 else car(lst) length = lambda lst, count=0: length(cdr(lst), count+1) if lst else count begin = lambda *args: args[-1] display = lambda lst: begin(w(\"%s \" % car(lst)), display(cdr(lst))) if lst else w(\"niln\") ```where `w = sys.stdout.write```` foldr = lambda f, i: lambda s: reduce(f, s, i) foldl = reduce mapcar = map Pay attention about the speed when you use Python, here is a benchmarks for 5 languages: from [The Great Computer Language Shootout](http://www.bagley.org/~doug/shootout/). Test Lisp Java Python Perl C++ hash access 1.06 3.23 4.01 1.85 1.00 exception handling 0.01 0.90 1.54 1.73 1.00 Legend sum numbers from file 7.54 2.63 8.34 2.49 1.00 \\\u003e 100 x C++ reverse lines 1.61 1.22 1.38 1.25 1.00 50-100 x C++ matrix multiplication 3.30 8.90 278.00 226.00 1.00 10-50 x C++ heapsort 1.67 7.00 84.42 75.67 1.00 5-10 x C++ array access 1.75 6.83 141.08 127.25 1.00 2-5 x C++ list processing 0.93 20.47 20.33 11.27 1.00 1-2 x C++ object instantiation 1.32 2.39 49.11 89.21 1.00 \u003c 1 x C++ word count 0.73 4.61 2.57 1.64 1.00 **Median** 1.67 4.61 20.33 11.27 1.00 **25% to 75%** 0.93 to 1.67 2.63 to 7.00 2.57 to 84.42 1.73 to 89.21 1.00 to 1.00 **Range** 0.01 to 7.54 0.90 to 20.47 1.38 to 278 1.25 to 226 1.00 to 1.00 Relative Resource: _[Python for Lisp Programmers](http://norvig.com/python-lisp.html)_ by _[Peter Norvig](http://www.norvig.com/)_ ","date":"2012-11-19","objectID":"/how-to-use-python-like-lisp/:0:0","tags":["Lisp","Python"],"title":"How to use Python like Lisp","uri":"/how-to-use-python-like-lisp/"},{"categories":null,"content":"Guanlan Dai Site Lead, China @Kong, formerly @Cloudflare. I’m particularly interested in Distributed Systems, Web Performance and Scalability, Data, Artificial Intelligence and Network Security. Feel free to check out my Github. My Technical Blogs In English http://rmmod.com/posts/  (Nov, 2012 ~ Present) In Chinese http://www.cppblog.com/xguru/  (Dec, 2009 ~ July, 2011) ","date":"2012-10-29","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]